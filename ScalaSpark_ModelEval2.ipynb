{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "860ba0bd-b068-45e5-b1b6-fc959554433e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://74e4be6e0134:4042\n",
       "SparkContext available as 'sc' (version = 3.2.1, master = local[*], app id = local-1651734057756)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.log4j._\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// To see less warnings\n",
    "import org.apache.log4j._\n",
    "Logger.getLogger(\"org\").setLevel(Level.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3b38814-fa55-4282-a689-d05e10167f6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
       "import org.apache.spark.ml.regression.LinearRegression\n",
       "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8f79ede-e05a-49c3-9948-28f0eee6095d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "import org.apache.spark.ml.regression.LinearRegression\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@4273a12b\n",
       "import spark.implicits._\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "\n",
    "val spark = SparkSession.builder()\n",
    "                        .appName(\"LinearRegEval1\")\n",
    "                        .config(\"spark.master\", \"local\")\n",
    "                        .getOrCreate()\n",
    "\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2d5264b-fae4-4213-85f0-c3ea40a2ce75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.VectorAssembler\n",
       "import org.apache.spark.ml.linalg.Vectors\n",
       "data: org.apache.spark.sql.DataFrame = [Avg Area Income: double, Avg Area House Age: double ... 4 more fields]\n",
       "df: org.apache.spark.sql.DataFrame = [label: double, Avg Area Income: double ... 3 more fields]\n",
       "df: org.apache.spark.sql.DataFrame = [label: double, Avg Area Income: double ... 3 more fields]\n",
       "df: org.apache.spark.sql.DataFrame = [label: double, Avg Area Income: double ... 3 more fields]\n",
       "df: org.apache.spark.sql.DataFrame = [label: double, Avg Area Income: double ... 3 more fields]\n",
       "df: org.apache.spark.sql.DataFrame = [label: double, Avg Area Income: double ... 3 more fields]\n",
       "df: org.apache.spark.sql.DataFrame = [label: double, Avg Area Income: double ... 3 more fields]\n",
       "df: org.apache.sp...\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Setting Up DataFrame for Machine Learning\n",
    "\n",
    "\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.linalg.Vectors\n",
    "\n",
    "\n",
    "// Prepare training and test data.\n",
    "val data = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").format(\"csv\").load(\"USA_Housing.csv\")\n",
    "\n",
    "\n",
    "// Grab only numerical columns from the data\n",
    "var df = data.select($\"Price\", $\"Avg Area Income\", $\"Avg Area House Age\", $\"Avg Area Number of Rooms\", $\"Area Population\")\n",
    "// Convert to double \n",
    "df = df.withColumn(\"Avg Area Income\", df(\"Avg Area Income\").cast(\"double\"))\n",
    "df = df.withColumn(\"Avg Area House Age\", df(\"Avg Area Income\").cast(\"double\"))\n",
    "df = df.withColumn(\"Avg Area Number of Rooms\", df(\"Avg Area Number of Rooms\").cast(\"double\"))\n",
    "df = df.withColumn(\"Area Population\", df(\"Area Population\").cast(\"double\"))\n",
    "\n",
    "df = df.withColumn(\"Price\", df(\"Price\").cast(\"double\"))\n",
    "\n",
    "df = df.withColumnRenamed(\"Price\", \"label\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "017d52e8-2cc1-4216-9357-e4d5e00aa64e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assembler: org.apache.spark.ml.feature.VectorAssembler = VectorAssembler: uid=vecAssembler_7dfb021af0b2, handleInvalid=error, numInputCols=4\n",
       "df1: org.apache.spark.sql.DataFrame = [label: double, features: vector]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// An assembler converts the input values to a vector\n",
    "// A vector is what the ML algorithm reads to train a model\n",
    "\n",
    "// Set the input columns from which we are supposed to read the values\n",
    "// Set the name of the column where the vector will be stored\n",
    "val assembler = new VectorAssembler().setInputCols(Array(\"Avg Area Income\",\"Avg Area House Age\",\"Avg Area Number of Rooms\",\"Area Population\")).setOutputCol(\"features\")\n",
    "\n",
    "// Use the assembler to transform our DataFrame to the two columns\n",
    "val df1 = assembler.transform(df).select($\"label\", $\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e49da87-40e3-4feb-9d2f-0a5d4db5e1c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: double, features: vector]\n",
       "test: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: double, features: vector]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(training, test) = df1.randomSplit(Array(0.7, 0.3), seed = 12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c7471a7-401d-4351-bfbb-e71060dd235c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+------------------+\n",
      "|            features|      label|        prediction|\n",
      "+--------------------+-----------+------------------+\n",
      "|[48904.98327,4890...|201898.0866| 563666.4093469658|\n",
      "|[49851.13478,4985...|283208.1322| 554043.4204602668|\n",
      "|[47685.25759,4768...|294170.7464|418509.62306271656|\n",
      "|[59801.49103,5980...|311111.2006| 748840.5853979208|\n",
      "|[59141.79644,5914...|313651.5032| 577790.0929296722|\n",
      "|[43952.33621,4395...| 324981.993| 569285.1399972925|\n",
      "|[58342.36779,5834...|365929.5973| 567490.5357962397|\n",
      "|[50847.11311,5084...|398909.5099| 621924.4535205176|\n",
      "|[52202.85372,5220...|404976.3659| 647514.5952812966|\n",
      "|[42814.99304,4281...|452530.1767| 637844.4839183541|\n",
      "|[59788.21893,5978...|461473.5665| 620147.7395165844|\n",
      "|[62639.15981,6263...|476971.4559| 785938.1802047442|\n",
      "|[66989.50469,6698...|490338.6033|  684365.693746607|\n",
      "|[55415.24226,5541...|492984.5947|472768.54618370044|\n",
      "|[41007.45867,4100...|494742.5436| 563661.6286209642|\n",
      "|[44806.00516,4480...|497579.4466| 756243.9538992511|\n",
      "|[58276.58119,5827...|502553.0522| 794553.9103191353|\n",
      "|[44247.14904,4424...|503065.5056|384325.11903101136|\n",
      "|[40185.73389,4018...|529282.0844| 484295.3037859958|\n",
      "|[46926.37115,4692...|530764.6725|  804926.916409977|\n",
      "+--------------------+-----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lr: org.apache.spark.ml.regression.LinearRegression = linReg_ef4ee4f9aea2\n",
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\tlinReg_ef4ee4f9aea2-regParam: 1000.0\n",
       "}, {\n",
       "\tlinReg_ef4ee4f9aea2-regParam: 0.001\n",
       "})\n",
       "trainValidationSplit: org.apache.spark.ml.tuning.TrainValidationSplit = tvs_d2dbcc67f195\n",
       "model: org.apache.spark.ml.tuning.TrainValidationSplitModel = TrainValidationSplitModel: uid=tvs_d2dbcc67f195, bestModel=LinearRegressionModel: uid=linReg_ef4ee4f9aea2, numFeatures=4, trainRatio=0.8\n",
       "res2: Array[Double] = Array(194769.4647996773, 194765.98554157064)\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// LINEAR REGRESSION \n",
    "val lr = new LinearRegression()\n",
    "\n",
    "\n",
    "// PARAMETER GRID BUILDER\n",
    "val paramGrid = new ParamGridBuilder().addGrid(lr.regParam,Array(1000,0.001)).build()\n",
    "\n",
    "// TRAIN TEST SPLIT\n",
    "\n",
    "// In this case the estimator is simply the linear regression.\n",
    "// A TrainValidationSplit requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "// 80% of the data will be used for training and the remaining 20% for validation.\n",
    "val trainValidationSplit = (new TrainValidationSplit()\n",
    "                            .setEstimator(lr)\n",
    "                            .setEvaluator(new RegressionEvaluator())\n",
    "                            .setEstimatorParamMaps(paramGrid)\n",
    "                            .setTrainRatio(0.8) )\n",
    "\n",
    "\n",
    "// You can then treat this object as the new model and use fit on it.\n",
    "// Run train validation split, and choose the best set of parameters.\n",
    "val model = trainValidationSplit.fit(training)\n",
    "\n",
    "\n",
    "// EVALUATION USING THE TEST DATA\n",
    "\n",
    "// Make predictions on test data. model is the model with combination of parameters\n",
    "// that performed best.\n",
    "model.transform(test).select(\"features\", \"label\", \"prediction\").show()\n",
    "\n",
    "// Check out the metrics\n",
    "model.validationMetrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02383d21-beeb-43d5-8d5a-51275b507444",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
