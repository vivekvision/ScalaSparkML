{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25dab0b0-860f-448d-93f6-c269d25dd474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://74e4be6e0134:4041\n",
       "SparkContext available as 'sc' (version = 3.2.1, master = local[*], app id = local-1651728432589)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.log4j._\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// To see less warnings\n",
    "import org.apache.log4j._\n",
    "Logger.getLogger(\"org\").setLevel(Level.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "298dc00f-cde3-4741-9b39-bf72a9375b07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
       "import org.apache.spark.ml.regression.LinearRegression\n",
       "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdf344ba-e9f3-4d84-9ccd-e7578ebf292a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "import org.apache.spark.ml.regression.LinearRegression\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@4b861c4a\n",
       "import spark.implicits._\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "\n",
    "val spark = SparkSession.builder()\n",
    "                        .appName(\"LinearRegEval\")\n",
    "                        .config(\"spark.master\", \"local\")\n",
    "                        .getOrCreate()\n",
    "\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ea5d2c1-6b6a-4f96-9d40-d7fd46875e3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: org.apache.spark.sql.DataFrame = [Avg Area Income: double, Avg Area House Age: double ... 4 more fields]\n",
       "import org.apache.spark.ml.feature.VectorAssembler\n",
       "import org.apache.spark.ml.linalg.Vectors\n",
       "df: org.apache.spark.sql.DataFrame = [label: double, Avg Area Income: double ... 3 more fields]\n",
       "df: org.apache.spark.sql.DataFrame = [label: double, Avg Area Income: double ... 3 more fields]\n",
       "df: org.apache.spark.sql.DataFrame = [label: double, Avg Area Income: double ... 3 more fields]\n",
       "df: org.apache.spark.sql.DataFrame = [label: double, Avg Area Income: double ... 3 more fields]\n",
       "df: org.apache.spark.sql.DataFrame = [label: double, Avg Area Income: double ... 3 more fields]\n",
       "df: org.apache.spark.sql.DataFrame = [label: double, Avg Area Income: double ... 3 more fields]\n",
       "df: org.apache.sp...\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Prepare training and test data.\n",
    "val data = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").format(\"csv\").load(\"USA_Housing.csv\")\n",
    "\n",
    "\n",
    "\n",
    "// This will allow us to join multiple feature columns\n",
    "// into a single column of an array of feautre values\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.linalg.Vectors\n",
    "\n",
    "\n",
    "// Grab only numerical columns from the data\n",
    "var df = data.select($\"Price\", $\"Avg Area Income\", $\"Avg Area House Age\", $\"Avg Area Number of Rooms\", $\"Area Population\")\n",
    "// Convert to double \n",
    "df = df.withColumn(\"Avg Area Income\", df(\"Avg Area Income\").cast(\"double\"))\n",
    "df = df.withColumn(\"Avg Area House Age\", df(\"Avg Area Income\").cast(\"double\"))\n",
    "df = df.withColumn(\"Avg Area Number of Rooms\", df(\"Avg Area Number of Rooms\").cast(\"double\"))\n",
    "df = df.withColumn(\"Area Population\", df(\"Area Population\").cast(\"double\"))\n",
    "\n",
    "df = df.withColumn(\"Price\", df(\"Price\").cast(\"double\"))\n",
    "\n",
    "df = df.withColumnRenamed(\"Price\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a53e8a8-aef2-40db-b946-3a9a3b8480f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assembler: org.apache.spark.ml.feature.VectorAssembler = VectorAssembler: uid=vecAssembler_f6915188549b, handleInvalid=error, numInputCols=4\n",
       "df1: org.apache.spark.sql.DataFrame = [label: double, features: vector]\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// An assembler converts the input values to a vector\n",
    "// A vector is what the ML algorithm reads to train a model\n",
    "\n",
    "// Set the input columns from which we are supposed to read the values\n",
    "// Set the name of the column where the vector will be stored\n",
    "val assembler = new VectorAssembler().setInputCols(Array(\"Avg Area Income\",\"Avg Area House Age\",\"Avg Area Number of Rooms\",\"Area Population\")).setOutputCol(\"features\")\n",
    "\n",
    "// Use the assembler to transform our DataFrame to the two columns\n",
    "val df1 = assembler.transform(df).select($\"label\", $\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f84a2ea8-78b0-4636-a823-701d66b26227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: double, features: vector]\n",
       "test: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: double, features: vector]\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(training, test) = df1.randomSplit(Array(0.9, 0.1), seed = 12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6afc4b43-3c5d-48f4-b727-f373c579e377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|      label|            features|\n",
      "+-----------+--------------------+\n",
      "|15938.65792|[47320.65721,4732...|\n",
      "|31140.51762|[37971.20757,3797...|\n",
      "|88591.77016|[60167.67261,6016...|\n",
      "|143027.3645|[35963.33081,3596...|\n",
      "+-----------+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "daab2543-5caa-43c4-a9d9-8ec5a39f6c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|      label|            features|\n",
      "+-----------+--------------------+\n",
      "|311111.2006|[59801.49103,5980...|\n",
      "|313651.5032|[59141.79644,5914...|\n",
      "|365929.5973|[58342.36779,5834...|\n",
      "|398909.5099|[50847.11311,5084...|\n",
      "+-----------+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f01ba647-6aa4-42a5-b719-73432c644407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+------------------+\n",
      "|            features|      label|        prediction|\n",
      "+--------------------+-----------+------------------+\n",
      "|[59801.49103,5980...|311111.2006| 754922.0464481027|\n",
      "|[59141.79644,5914...|313651.5032| 587039.9686413496|\n",
      "|[58342.36779,5834...|365929.5973| 576055.3632130935|\n",
      "|[50847.11311,5084...|398909.5099| 628280.4269229695|\n",
      "|[52202.85372,5220...|404976.3659| 654193.7257026793|\n",
      "|[41007.45867,4100...|494742.5436| 568822.9041552809|\n",
      "|[44806.00516,4480...|497579.4466| 759559.0305158193|\n",
      "|[44247.14904,4424...|503065.5056|393037.18210031977|\n",
      "|[45353.13038,4535...|544320.6881| 739439.1460034964|\n",
      "|[51510.69878,5151...|549167.9399| 755447.4889441873|\n",
      "|[67186.26237,6718...|554702.6802| 588802.7504048888|\n",
      "|[44328.2563,44328...|601007.3512| 455471.2135610604|\n",
      "|[59777.37269,5977...|614700.7371| 910251.8477743613|\n",
      "|[57925.0447,57925...|633875.9302| 1115395.441577265|\n",
      "|[57576.05395,5757...|634218.4408| 822872.4896821738|\n",
      "|[52022.15897,5202...|642855.1696| 726039.5952844103|\n",
      "|[50442.26643,5044...|644142.4757| 829065.2881163964|\n",
      "|[52046.54354,5204...|674657.9107| 785456.1744317557|\n",
      "|[76907.23816,7690...|679228.9927|1130087.7582107014|\n",
      "|[59037.60924,5903...|685922.3852| 962177.0826813383|\n",
      "+--------------------+-----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lr: org.apache.spark.ml.regression.LinearRegression = linReg_cec1b43155fa\n",
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\tlinReg_cec1b43155fa-elasticNetParam: 0.0,\n",
       "\tlinReg_cec1b43155fa-fitIntercept: true,\n",
       "\tlinReg_cec1b43155fa-regParam: 0.1\n",
       "}, {\n",
       "\tlinReg_cec1b43155fa-elasticNetParam: 0.0,\n",
       "\tlinReg_cec1b43155fa-fitIntercept: true,\n",
       "\tlinReg_cec1b43155fa-regParam: 0.01\n",
       "}, {\n",
       "\tlinReg_cec1b43155fa-elasticNetParam: 0.0,\n",
       "\tlinReg_cec1b43155fa-fitIntercept: false,\n",
       "\tlinReg_cec1b43155fa-regParam: 0.1\n",
       "}, {\n",
       "\tlinReg_cec1b43155fa-elasticNetParam: 0.0,\n",
       "\tlinReg_cec1b43155fa-fitIntercept: false,\n",
       "\tlinReg_cec1b43155fa-regParam: 0.01\n",
       "}, {\n",
       "\tlinReg_cec1b43155fa-elasticNetParam: 0.5,\n",
       "\tlinReg_cec1b43155fa-fitIntercept: true,\n",
       "\tlinReg_cec1b43155fa-regParam: 0.1\n",
       "}, {\n",
       "\tlinReg_cec1b43155fa-elasticNetParam...\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// param grid\n",
    "\n",
    "\n",
    "val lr = new LinearRegression()\n",
    "\n",
    "// We use a ParamGridBuilder to construct a grid of parameters to search over.\n",
    "// TrainValidationSplit will try all combinations of values and determine best model using\n",
    "// the evaluator.\n",
    "val paramGrid = new ParamGridBuilder().addGrid(lr.regParam, Array(0.1, 0.01)).addGrid(lr.fitIntercept).addGrid(lr.elasticNetParam, Array(0.0, 0.5, 1.0)).build()\n",
    "\n",
    "// In this case the estimator is simply the linear regression.\n",
    "// A TrainValidationSplit requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "// 80% of the data will be used for training and the remaining 20% for validation.\n",
    "val trainValidationSplit = new TrainValidationSplit().setEstimator(lr).setEvaluator(new RegressionEvaluator).setEstimatorParamMaps(paramGrid).setTrainRatio(0.8)\n",
    "\n",
    "\n",
    "// Run train validation split, and choose the best set of parameters.\n",
    "val model = trainValidationSplit.fit(training)\n",
    "\n",
    "// Make predictions on test data. model is the model with combination of parameters\n",
    "// that performed best.\n",
    "model.transform(test).select(\"features\", \"label\", \"prediction\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90c729b-c4a7-4b5f-a071-34886fc6f680",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
